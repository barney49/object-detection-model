{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barney49/object-detection-model/blob/main/FRAN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data with Kaggle API and Install Libraries"
      ],
      "metadata": {
        "id": "LDHrJWLqTslh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VdNTcbDnwAdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zZ4veVuxwAbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dxetklgZwAZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload your kaggle.json to directory to download the data or download here [aged-synthetic-dataset](https://www.kaggle.com/datasets/penpentled/aged-synthetic-images)"
      ],
      "metadata": {
        "id": "yMaRPjXG3HyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "WYxzZARR29RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ead23d3-d5fc-4aad-e7b5-e588f7204b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d penpentled/aged-synthetic-images\n",
        "!unzip /content/aged-synthetic-images.zip -d /content/"
      ],
      "metadata": {
        "id": "ku6CL1Mh5C7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748dae11-a6d1-4c4d-bffc-4d7a4bb37afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open /content/aged-synthetic-images.zip, /content/aged-synthetic-images.zip.zip or /content/aged-synthetic-images.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install piq kornia pytorch-lightning"
      ],
      "metadata": {
        "id": "a2SuM6KG5iaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2940e038-0c1e-437f-e3e5-d2c4402cbaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting piq\n",
            "  Downloading piq-0.8.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9.post0-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from piq) (0.15.2+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (16.0.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->piq) (9.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, piq, kornia\n",
            "Successfully installed kornia-0.7.0 lightning-utilities-0.9.0 piq-0.8.0 pytorch-lightning-2.0.9.post0 torchmetrics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-D3HJNlavygs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_O5zyFivzLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cd8MoToR7woh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9m3pDIK8EEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppO0P3Uo8EU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Dataset"
      ],
      "metadata": {
        "id": "A3NP4YTHhpFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import kornia\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import random\n",
        "from itertools import permutations\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "P9unxXlhq5cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_AGES = [18,23,28,33,38,43,48,53,58,63,68,73,78,83]\n",
        "DATA_DIR = '/content/synthetic_images/'"
      ],
      "metadata": {
        "id": "_eKl-g8is8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FRANDataset(Dataset):\n",
        "  def __init__(self, image_pairs, image_ages_dict, transforms, transform_resize):\n",
        "    self.image_pairs = image_pairs\n",
        "    self.image_ages_dict = image_ages_dict\n",
        "    self.transforms = transform_normalize\n",
        "    self.transform_resize = transform_resize\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_pair = self.image_pairs[index]\n",
        "\n",
        "    input_image = np.array(Image.open(DATA_DIR+image_pair[0]).convert('RGB'))\n",
        "    target_image = np.array(Image.open(DATA_DIR+image_pair[1]).convert('RGB'))\n",
        "\n",
        "    normalized_input_image_transformed = self.transforms(image=input_image)\n",
        "    normalized_input_image = normalized_input_image_transformed['image']/127.5 - 1\n",
        "\n",
        "    normalized_target_image = A.ReplayCompose.replay(normalized_input_image_transformed['replay'], image=np.array(target_image))['image']/127.5 - 1\n",
        "\n",
        "    rgb_delta = normalized_target_image - normalized_input_image\n",
        "\n",
        "    # Get age maps\n",
        "    _,width,height=normalized_input_image.shape\n",
        "    age_map1 = torch.full((1,width,height),self.image_ages_dict[image_pair[0]]/100)\n",
        "    age_map2 = torch.full((1,width,height),self.image_ages_dict[image_pair[1]]/100)\n",
        "\n",
        "    # Combine RGB delta diff with age maps for 5-channel tensor\n",
        "    input_tensor = torch.cat((normalized_input_image, age_map1, age_map2), dim=0)\n",
        "\n",
        "    return {\n",
        "        'input': input_tensor,\n",
        "        'normalized_input_image': normalized_input_image,\n",
        "        'normalized_target_image': normalized_target_image,\n",
        "        'target_age': age_map2,\n",
        "      }"
      ],
      "metadata": {
        "id": "wZ-eYYvVqXsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some augmentations\n",
        "transform_normalize = A.ReplayCompose([\n",
        "    A.Resize(512, 512),\n",
        "    A.RandomCrop(256, 256),\n",
        "    A.ColorJitter(p=0.8),\n",
        "    A.Blur(p=0.1),\n",
        "    A.RandomBrightnessContrast(),\n",
        "    A.Affine(rotate=[-30, 30],scale=(0.5,1.5), p=0.8),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "\n",
        "transform_resize = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "Cec_NCcIqhyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_meta = pd.read_csv('/content/image_meta.csv')"
      ],
      "metadata": {
        "id": "JEpbQoquq1-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "1ed6bdaa-7daa-4c10-c160-26e2a6f452ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9362ee72f161>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/image_meta.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/image_meta.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_meta"
      ],
      "metadata": {
        "id": "roCkGl9Fq8gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate image age pairs\n",
        "image_pairs = []\n",
        "\n",
        "for _, row in image_meta.iterrows():\n",
        "    image_paths = row.tolist()  # Convert the row to a list of image paths\n",
        "    image_combinations = permutations(image_paths, 2)\n",
        "    image_pairs.extend(list(image_combinations))\n",
        "\n",
        "image_pairs[:5]"
      ],
      "metadata": {
        "id": "vdsMMnyXrjR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate image age dict {image path:age}\n",
        "image_age_dict={}\n",
        "for target_age, column in zip(TARGET_AGES,image_meta.columns):\n",
        "  image_age_dict.update({path:target_age for path in image_meta[column].values})"
      ],
      "metadata": {
        "id": "1LMj02U_snRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "ZItWQlaA41XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BlurUpSample(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super(BlurUpSample, self).__init__()\n",
        "        self.blurpool =  kornia.filters.GaussianBlur2d((3, 3), (1.5, 1.5))\n",
        "        self.upsample = nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blurpool(x)\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DownLayer(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(DownLayer, self).__init__()\n",
        "        self.maxblurpool = kornia.filters.MaxBlurPool2D(kernel_size=3)\n",
        "        self.conv1 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(c_out)\n",
        "        self.leakyrelu = nn.LeakyReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxblurpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpLayer(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(UpLayer, self).__init__()\n",
        "        self.upsample = BlurUpSample(c_in)\n",
        "        self.conv1 = nn.Conv2d(c_in+ c_out, c_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(c_out)\n",
        "        self.leakyrelu = nn.LeakyReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(c_out)\n",
        "\n",
        "    def forward(self, x, skip_x):\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        dh = skip_x.size(2) - x.size(2)\n",
        "        dw = skip_x.size(3) - x.size(3)\n",
        "\n",
        "        x = F.pad(x, (dw // 2, dw - dw // 2, dh // 2, dh - dh // 2))\n",
        "\n",
        "        x = torch.cat([x, skip_x], dim=1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EISTLHBchtMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FRAN Unet Model\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(5, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "    self.leakyrelu = nn.LeakyReLU(inplace=True)\n",
        "    self.downlayer1 = DownLayer(64, 128)\n",
        "    self.downlayer2 = DownLayer(128, 256)\n",
        "    self.downlayer3 = DownLayer(256, 512)\n",
        "    self.downlayer4 = DownLayer(512, 1024)\n",
        "    self.uplayer1 = UpLayer(1024, 512)\n",
        "    self.uplayer2 = UpLayer(512, 256)\n",
        "    self.uplayer3 = UpLayer(256, 128)\n",
        "    self.uplayer4 = UpLayer(128, 64)\n",
        "    self.conv3 = nn.Conv2d(64, 3, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(f'Input Shape: {x.shape}')\n",
        "    x1 = self.conv1(x)\n",
        "    x1 = self.batchnorm1(x1)\n",
        "    x1 = self.leakyrelu(x1)\n",
        "    x1 = self.conv2(x1)\n",
        "    x1 = self.batchnorm1(x1)\n",
        "    x1 = self.leakyrelu(x1)\n",
        "\n",
        "    #print(f'Processed Input Shape: {x.shape}')\n",
        "\n",
        "    x2 = self.downlayer1(x1)\n",
        "    x3 = self.downlayer2(x2)\n",
        "    x4 = self.downlayer3(x3)\n",
        "    x5 = self.downlayer4(x4)\n",
        "\n",
        "    #print(f'Done Downlayering... Shape: {x5.shape}')\n",
        "\n",
        "    x = self.uplayer1(x5, x4)\n",
        "    x = self.uplayer2(x, x3)\n",
        "    x = self.uplayer3(x, x2)\n",
        "    x = self.uplayer4(x, x1)\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    #print(f'Output Shape: {x.shape}')\n",
        "    return x"
      ],
      "metadata": {
        "id": "B5XRg_Azj6Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchGANDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PatchGANDiscriminator, self).__init__()\n",
        "        self.conv0 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.leakyrelu = nn.LeakyReLU(inplace=True)\n",
        "        self.maxblurpool = kornia.filters.MaxBlurPool2D(kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.maxblurpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.maxblurpool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.maxblurpool(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ze8q9iZykHwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "XhYS935exTz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from piq import LPIPS"
      ],
      "metadata": {
        "id": "7lfMneh5y9Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "P_5DkonJ7mQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "m0g_mspi2tNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = FRANDataset(image_pairs, image_age_dict, transform_normalize=transform_normalize, transform_resize=transform_resize)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "nt4HpcE82VnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import BCEWithLogitsLoss\n",
        "# Define losses\n",
        "adversarial_loss = BCEWithLogitsLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "perceptual_loss = LPIPS().to(device)"
      ],
      "metadata": {
        "id": "Je5PovLmWLes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_l1 = 1\n",
        "lambda_perceptual = 1\n",
        "lambda_adversarial = 0.05"
      ],
      "metadata": {
        "id": "bJ1bNeZz69YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic pl lightning module for checkpointing and logging\n",
        "class FRAN(pl.LightningModule):\n",
        "  def __init__(self, generator, discriminator):\n",
        "    super(FRAN, self).__init__()\n",
        "    self.generator = generator\n",
        "    self.discriminator =discriminator\n",
        "    self.automatic_optimization = False\n",
        "    self.my_step =0\n",
        "\n",
        "  def forward(self, x):\n",
        "      with torch.no_grad():\n",
        "        model_output = self.generator(x)\n",
        "      return model_output\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "      opt_g, opt_d = self.optimizers()\n",
        "\n",
        "      opt_g.zero_grad()\n",
        "      opt_d.zero_grad()\n",
        "\n",
        "\n",
        "      inputs = batch['input'].to(self.device)\n",
        "      normalized_input_image = batch['normalized_input_image'].to(self.device)\n",
        "      normalized_target_image = batch['normalized_target_image'].to(self.device)\n",
        "\n",
        "\n",
        "      target_age = batch['target_age'].to(self.device)\n",
        "      # Forward pass\n",
        "      outputs = self.generator(inputs)\n",
        "\n",
        "      predicted_images = normalized_input_image+outputs\n",
        "      predicted_images_with_age = torch.cat((predicted_images, target_age), dim=1)\n",
        "\n",
        "      real_labels = torch.ones(inputs.shape[0], 1, 32, 32).to(self.device)\n",
        "      fake_labels = torch.zeros(inputs.shape[0], 1, 32, 32).to(self.device)\n",
        "\n",
        "      # Compute discriminator losses\n",
        "      real_loss = adversarial_loss(self.discriminator(torch.cat((normalized_target_image, target_age), dim=1)), real_labels)\n",
        "      fake_loss = adversarial_loss(self.discriminator(predicted_images_with_age.detach()), fake_labels)\n",
        "\n",
        "      d_loss = (real_loss+fake_loss) / 2\n",
        "\n",
        "      self.manual_backward(d_loss)\n",
        "      opt_d.step()\n",
        "\n",
        "      # Compute generator loss\n",
        "      l1_loss_value = l1_loss(predicted_images, normalized_target_image)\n",
        "      perceptual_loss_value = perceptual_loss(predicted_images, normalized_target_image)\n",
        "      adversarial_loss_value = adversarial_loss(self.discriminator(predicted_images_with_age), real_labels)\n",
        "\n",
        "      total_loss =  lambda_adversarial*adversarial_loss_value+ lambda_perceptual*perceptual_loss_value + lambda_l1*l1_loss_value\n",
        "\n",
        "      self.manual_backward(total_loss)\n",
        "      opt_g.step()\n",
        "\n",
        "      # Log loss\n",
        "      self.log('fake_loss', fake_loss, prog_bar=True)\n",
        "      self.log('discriminator_loss', d_loss, prog_bar=True)\n",
        "      self.log('total_loss', total_loss, prog_bar=True)\n",
        "      self.log('gen_adversarial_loss', lambda_adversarial*adversarial_loss_value, prog_bar=True)\n",
        "      self.log('perceptual_loss', perceptual_loss_value.mean()*lambda_perceptual, prog_bar=True)\n",
        "      self.log('l1_loss', l1_loss_value*lambda_l1, prog_bar=True)\n",
        "\n",
        "      # Display images every 500 steps\n",
        "      if self.my_step % 100 == 0:\n",
        "\n",
        "        sample_image = inputs[0]\n",
        "        model_output = outputs.detach()\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 5, 1)\n",
        "        plt.imshow(((normalized_input_image[0].cpu().permute(1,2,0).numpy()+1)*127.5).astype('uint'))\n",
        "        plt.title(f\"Input Image, Age: {int(sample_image[3][0][0]*100)}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 5, 2)\n",
        "        plt.imshow(((normalized_target_image[0].cpu().permute(1,2,0).numpy()+1)*127.5).astype('uint'))\n",
        "        plt.title(\"Target Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        new_image = (((normalized_input_image[0].cpu()+model_output[0].cpu()).permute(1,2,0).numpy()+1)*127.5).astype('uint')\n",
        "        plt.subplot(1, 5, 3)\n",
        "        plt.imshow(new_image)\n",
        "        plt.title(f\"Output Image, Age: {int(sample_image[4][0][0]*100)}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        new_image = (((torch.abs(normalized_target_image[0].cpu()-normalized_input_image[0].cpu())).permute(1,2,0).numpy()+1)*127.5).astype('uint')\n",
        "        plt.subplot(1, 5, 4)\n",
        "        plt.imshow(new_image)\n",
        "        plt.title(f\"Tar RGB Diff\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        new_image = (((torch.abs(model_output[0].cpu())).permute(1,2,0).numpy()+1)*127.5).astype('uint')\n",
        "        plt.subplot(1, 5, 5)\n",
        "        plt.imshow(new_image)\n",
        "        plt.title(f\"Pred RGB Diff\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "      self.my_step +=1\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    generator_optimizer = optim.Adam(self.generator.parameters(), lr=0.0001)\n",
        "    discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "    return [generator_optimizer, discriminator_optimizer]"
      ],
      "metadata": {
        "id": "qfItxvPX1r-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fran_model = FRAN(Generator(), PatchGANDiscriminator())"
      ],
      "metadata": {
        "id": "ZtLOL5MB7IKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fran_trainer = pl.Trainer(\n",
        "  precision='16-mixed',\n",
        "devices=1,\n",
        "  max_epochs=20,\n",
        "  callbacks =[ pl.callbacks.ModelCheckpoint(\n",
        "    every_n_train_steps=5000,\n",
        "    dirpath='/content/age-model-checkpoints/',\n",
        "    filename='fran-{step:05d}',\n",
        "  )]\n",
        ")"
      ],
      "metadata": {
        "id": "okKH2ZNQ7nmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "77edd121-5c97-431b-96f5-93098f174c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-479af258509e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m fran_trainer = pl.Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'16-mixed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   callbacks =[ pl.callbacks.ModelCheckpoint(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fran_trainer.fit(fran_model, dataloader)"
      ],
      "metadata": {
        "id": "PhJ6mlFu9zCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ended training early. Ideally, train for over 3 epochs."
      ],
      "metadata": {
        "id": "LXijQ834Gufp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "khw8CY6HArD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Demo: Crop out face and run the image through the model. https://huggingface.co/spaces/penpen/age-transformation  "
      ],
      "metadata": {
        "id": "_nhu5qXRFzrF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LDHrJWLqTslh",
        "HsbUETZ9R6DG"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}